# Interpreting &chi;<sup>2</sup>

Computing the &chi;<sup>2</sup> value is useful for determining whether a model is consistent with a data set within its errors. Most (astro)physicists define &chi;<sup>2</sup> as &Sigma;<sub>i</sub>(O<sub>i</sub> - E<sub>i</sub>)<sup>2</sup>/&sigma;<sub>i</sub><sup>2</sup> where O<sub>i</sub>, E<sub>i</sub>, and &sigma;<sub>i</sub> are the Observed and Expected values and the errors. In other words, the numerator represents the actual residuals between the data and the model, and the denominator represents the expected residuals assuming Gaussian-distributed errors. (Note however that in online discussions, &chi;<sup>2</sup> is generally defined with an E<sub>i</sub> in the denominator, which represents the special case of Poisson-distributed data.) 

To see how the &chi;<sup>2</sup> value can serve as a test, consider that if the model is correct and the errors have been correctly estimated, &chi;<sup>2</sup> ≈ N, where N is the number of degrees of freedom (number of data points minus number of parameters in the model). Therefore scientists often speak loosely and say that if the “reduced” Chi-squared defined as &chi;<sup>2</sup>/N is approximately equal to 1, then the fit is good. But let’s take a closer look with. Download [this code](https://github.com/galastrostats/general/blob/master/interpretingchi2.py) and run it under Anaconda to complete the following steps. Actually read the code and answer the questions embedded in it as comments.

(a) Using Monte Carlo methods, create 1000 fake data sets following the underlying functional form y=1/x for x = 1, 2, 3…30 with Gaussian random errors on y of amplitude 0.1. Each data set defines one value of &chi;<sup>2</sup> by its residuals from the function y=1/x, and the 1000 values of &chi;<sup>2</sup> from all of the data sets can be divided by N and binned into a histogram to show you the reduced &chi;<sup>2</sup> distribution, which is a well-defined function analogous to a Gaussian or any other function. Note that y=1/x has no free parameters, so N is just the # of data points, 30.

(b) Now create 1000 fake data sets each with 300 values of x = 1.1, 1.2, 1.3… 30.9, using the same function y=1/x with the same errors of 0.1 on y. Overplot the new histogram of reduced &chi;<sup>2</sup> values for N=300. Is a reduced &chi;<sup>2</sup> of 1.3 equally good for both data sets? Google the functional form of the &chi;<sup>2</sup> distribution on the web to understand why in mathematical terms.

(c) This exercise shows that just knowing that the reduced &chi;<sup>2</sup> ≈ 1 does not tell you how good your model is. You must know N. If you do, you can compute confidence levels by integrating the probability under the normalized &chi;<sup>2</sup> distribution up to your measured &chi;<sup>2</sup>. Use np.argsort to do this approximately with the &chi;<sup>2</sup> distributions from your Monte Carlo.

If you get really stuck, consult [these solutions](https://github.com/galastrostats/general/blob/master/interpretingchi2.py.solns), but not 'til your puzzler is sore.
